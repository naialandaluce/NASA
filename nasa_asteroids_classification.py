# -*- coding: utf-8 -*-
"""NASA Asteroids Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lBvm_y-75Qu_A7qtbE9xeR_ofAEZ4yPb

### **Análisis del Dataset NASA Asteroids**

# **1. Estructura General del Dataset**

  Total de filas: 4,687 registros de asteroides

  Total de columnas: 40 variables que describen características de los asteroides
  
  Variable objetivo: "Hazardous" (booleano) - Indica si el asteroide es potencialmente peligroso
"""

# Importar librerías necesarias
import pandas as pd #data processing
import numpy as np ##linear algebra
import requests

import csv
from pandas import DataFrame ## reading data as tables
import matplotlib.pyplot as plt
import seaborn as sns

import IPython.display as display
from IPython.display import HTML

"""## Data cleaning"""

# Cargar el dataset
df = pd.read_csv('/content/drive/MyDrive/NASA Asteroids Classification/nasa.csv')

# Información básica sobre el dataset
##Are inappropiate values? (after pront I can seethere are no inappropiate values)
print("\nInformación básica del dataset:")
df.info()

# Verificar valores nulos (after the observation we can see that there is no missing data or null values in the collected data)
print("\nValores nulos por columna:")
df.isnull().sum().head()

"""## Data Distribution"""

df.describe() ##description of data

pd.set_option('display.max_columns', None)  # Mostrar todas las columnas
pd.set_option('display.width', 1000)        # Ancho de la pantalla
pd.set_option('display.max_rows', 20)       # Limitar a 20 filas para mejor visualización

# Mostrar las primeras filas con todas las columnas
display.display(HTML(df.head(10).to_html()))

"""# **2.- Multiclass Classification Problems**

##1.Clasificación Multiclase de Asteroides NASA
    Internet sobre cada caracteristica orgitales:
    Clasifica asteroides en 7 categorías según sus características orbitales:
    - *Earth-Crosser:* Cruzan la órbita de la Tierra con excentricidad moderada
    - *Earth-Crosser-HighEcc:* Cruzan la órbita de la Tierra con alta excentricidad
    - *Mars-Crosser:* Entre la Tierra y Marte con excentricidad moderada
    - *Mars-Crosser-HighEcc:* Entre la Tierra y Marte con alta excentricidad
    - *MainBelt-Low: *Cinturón principal con baja inclinación
    - *MainBelt-High:* Cinturón principal con alta inclinación
    - *Outer-Solar:* Más allá del cinturón principal
"""

# variable objetivo multiclase (OrbitalType) basada en características orbitales
def classify_asteroid(row):

    semi_major_axis = row['Semi Major Axis']
    eccentricity = row['Eccentricity']
    inclination = row['Inclination']

    # Clasificación por tipo orbital
    if semi_major_axis < 1.3:  # Aten y Apollo (cruzan la órbita terrestre)
        if eccentricity >= 0.4:
            return "Earth-Crosser-HighEcc"
        else:
            return "Earth-Crosser"
    elif semi_major_axis >= 1.3 and semi_major_axis < 2:  # Entre Tierra y Marte
        if eccentricity >= 0.4:
            return "Mars-Crosser-HighEcc"
        else:
            return "Mars-Crosser"
    elif semi_major_axis >= 2 and semi_major_axis < 3.3:  # Cinturón principal
        if inclination < 15:
            return "MainBelt-Low"
        else:
            return "MainBelt-High"
    else:  # Más allá del cinturón principal
        return "Outer-Solar"

# Aplicar la función de clasificación a cada asteroide
df['OrbitalType'] = df.apply(classify_asteroid, axis=1)

# Mostrar las primeras filas del dataset con la nueva columna
print("\nPrimeras filas con la nueva variable objetivo 'OrbitalType':")
print(df.head())

# Analizar la distribución de la nueva variable objetivo
class_counts = df['OrbitalType'].value_counts()
class_percentages = df['OrbitalType'].value_counts(normalize=True) * 100

# Mostrar la distribución de clases
distribution_df = pd.DataFrame({
    'Count': class_counts,
    'Percentage': class_percentages
})

print("\nDistribución de clases de la variable objetivo 'OrbitalType':")
print(distribution_df)

# Visualizar la distribución de clases
plt.figure(figsize=(12, 6))
ax = sns.barplot(x=class_counts.index, y=class_counts.values)
plt.title('Distribución de Tipos Orbitales de Asteroides', fontsize=15)
plt.xlabel('Tipo Orbital', fontsize=12)
plt.ylabel('Número de Asteroides', fontsize=12)
plt.xticks(rotation=45)

# Añadir etiquetas de porcentaje encima de cada barra
for i, p in enumerate(ax.patches):
    height = p.get_height()
    ax.text(
        p.get_x() + p.get_width()/2.,
        height + 20,
        f'{class_percentages.iloc[i]:.2f}%',
        ha="center"
    )

plt.tight_layout()
plt.show()

# Análisis de características por clase
print("\nEstadísticas por tipo orbital:")
class_stats = df.groupby('OrbitalType').agg({
    'Absolute Magnitude': 'mean',
    'Est Dia in KM(max)': 'mean',
    'Eccentricity': 'mean',
    'Inclination': 'mean'
}).round(4)

print(class_stats)

# Visualizar las características por clase
fig, axes = plt.subplots(2, 2, figsize=(16, 10))
fig.suptitle('Características por Tipo Orbital', fontsize=16)

# Diagrama de caja para Magnitud Absoluta
sns.boxplot(x='OrbitalType', y='Absolute Magnitude', data=df, ax=axes[0, 0])
axes[0, 0].set_title('Magnitud Absoluta por Tipo Orbital')
axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45)

# Diagrama de caja para Diámetro
sns.boxplot(x='OrbitalType', y='Est Dia in KM(max)', data=df, ax=axes[0, 1])
axes[0, 1].set_title('Diámetro Estimado (km) por Tipo Orbital')
axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45)

# Diagrama de caja para Excentricidad
sns.boxplot(x='OrbitalType', y='Eccentricity', data=df, ax=axes[1, 0])
axes[1, 0].set_title('Excentricidad por Tipo Orbital')
axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)

# Diagrama de caja para Inclinación
sns.boxplot(x='OrbitalType', y='Inclination', data=df, ax=axes[1, 1])
axes[1, 1].set_title('Inclinación (grados) por Tipo Orbital')
axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

# Guardar el dataset con la nueva variable objetivo
df.to_csv('/content/nasa_with_orbital_types.csv', index=False)
print("\nDataset con variable objetivo guardado como 'nasa_with_orbital_types.csv'")

# Visualizar algunos ejemplos de cada clase
print("\nEjemplos de asteroides por clase:")
for class_name in class_counts.index:
    print(f"\nClase: {class_name}")
    print(df[df['OrbitalType'] == class_name].head(2))

"""## 2. Clasificación por Nivel de Riesgo
Esta clasificación combinaría múltiples factores para crear una evaluación integral del riesgo potencial:

    Clasifica asteroides en 6 categorías según múltiples factores de riesgo:
    - Negligible: Muy pequeño y/o distante
    - Very Low: Pequeño y distante
    - Low: Tamaño moderado o acercamiento moderado
    - Moderate: Combinación de factores de riesgo intermedios
    - High: Grande y cercano
    - Extreme: Combinación de factores de alto riesgo

"""

def classify_by_risk(row):
    diameter = row['Est Dia in KM(max)']
    miss_distance = row['Miss Dist.(Astronomical)']
    velocity = row['Relative Velocity km per sec']

    # Factor de tamaño (0-4)
    if diameter < 0.05:
        size_factor = 0
    elif diameter < 0.2:
        size_factor = 1
    elif diameter < 1.0:
        size_factor = 2
    elif diameter < 5.0:
        size_factor = 3
    else:
        size_factor = 4

    # Factor de distancia (0-4)
    if miss_distance > 0.3:
        distance_factor = 0
    elif miss_distance > 0.1:
        distance_factor = 1
    elif miss_distance > 0.05:
        distance_factor = 2
    elif miss_distance > 0.01:
        distance_factor = 3
    else:
        distance_factor = 4

    # Factor de velocidad (0-2)
    if velocity < 10:
        velocity_factor = 0
    elif velocity < 20:
        velocity_factor = 1
    else:
        velocity_factor = 2

    # Puntuación de riesgo combinada
    risk_score = size_factor + distance_factor + velocity_factor

    # Clasificación por puntuación
    if risk_score <= 2:
        return "Negligible"
    elif risk_score <= 4:
        return "Very Low"
    elif risk_score <= 6:
        return "Low"
    elif risk_score <= 8:
        return "Moderate"
    elif risk_score <= 10:
        return "High"
    else:
        return "Extreme"

# Aplicar la clasificación
df['RiskClass'] = df.apply(classify_by_risk, axis=1)

# Analizar la distribución de
class_counts = df['RiskClass'].value_counts()
class_percentages = df['RiskClass'].value_counts(normalize=True) * 100

# Mostrar la distribución de clases
distribution_df = pd.DataFrame({
    'Count': class_counts,
    'Percentage': class_percentages
})

print("\nDistribución de clases de la variable objetivo 'RiskClass':")
print(distribution_df)

"""## 3. Clasificación por Potencial Científico
Esta clasificación se centraría en el interés científico potencial del asteroide:

    Clasifica asteroides por su potencial interés científico:
    - Common: Asteroide típico
    - Eccentric: Órbita inusual
    - FastRotator: Rotación rápida inferida
    - Pristine: Potencialmente prístino
    - Metallic: Potencialmente metálico
    - Peculiar: Características poco comunes

"""

def classify_by_scientific_potential(row):
    # Define characteristics that make asteroids scientifically interesting
    has_unusual_orbit = (row['Eccentricity'] > 0.7 or row['Inclination'] > 40)
    has_unusual_composition = (row['Absolute Magnitude'] < 16 or row['Est Dia in KM(max)'] > 5)
    potentially_pristine = (row['Aphelion Dist'] > 3.5 and row['Perihelion Distance'] > 2.5)
    potentially_metallic = (row['Absolute Magnitude'] > 15 and row['Absolute Magnitude'] < 18)
    fast_rotation = (row['Mean Motion'] > 0.9)

    # Classify based on characteristics
    if potentially_pristine:
        return "Pristine"
    elif potentially_metallic:
        return "Metallic"
    elif fast_rotation:
        return "FastRotator"
    elif has_unusual_orbit:
        return "Eccentric"
    elif has_unusual_composition:
        return "Peculiar"
    else:
        return "Common"

# Apply classification
df['ScientificPotential'] = df.apply(classify_by_scientific_potential, axis=1)

# Analizar la distribución de
class_counts = df['ScientificPotential'].value_counts()
class_percentages = df['ScientificPotential'].value_counts(normalize=True) * 100

# Mostrar la distribución de clases
distribution_df = pd.DataFrame({
    'Count': class_counts,
    'Percentage': class_percentages
})

print("\nDistribución de clases de la variable objetivo 'ScientificPotential':")
print(distribution_df)

"""# **3 Data Exploration and Preprocessing**

# - Feature Elimination Strategy Implementation
"""

# Calculate correlation matrix
corr_matrix = df.select_dtypes(include=['float64', 'int64']).corr().abs()

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Plot correlation matrix
plt.figure(figsize=(20, 16))
sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', vmax=1.0, vmin=0,
            annot=False, square=True, linewidths=.5)
plt.title('Feature Correlation Matrix', fontsize=20)
plt.tight_layout()
plt.show()

# Find highly correlated feature pairs (above threshold)
threshold = 0.8
high_corr_features = []

for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if corr_matrix.iloc[i, j] > threshold:
            high_corr_features.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))

# Display highly correlated pairs
high_corr_df = pd.DataFrame(high_corr_features, columns=['Feature 1', 'Feature 2', 'Correlation'])
high_corr_df = high_corr_df.sort_values('Correlation', ascending=False)
print("Highly correlated feature pairs:")
print(high_corr_df)

"""# - standardize your data first, then apply the feature elimination strategy:"""

from sklearn.preprocessing import StandardScaler
# First, let's isolate just the numerical features for scaling
numeric_features = df.select_dtypes(include=['float64', 'int64']).copy()

# Save categorical columns for later
categorical_cols = [col for col in df.columns if col not in numeric_features.columns]

#Apply StandardScaler
scaler = StandardScaler()
scaled_features = scaler.fit_transform(numeric_features)
scaled_df = pd.DataFrame(scaled_features, columns=numeric_features.columns)

# Now, implement the feature elimination based on correlation

# Level 1: Remove highly correlated measurement features
# Keep only one type of diameter measurement (kilometers) and remove others
measurement_features_to_remove = [
    'Est Dia in M(min)', 'Est Dia in M(max)',
    'Est Dia in Miles(min)', 'Est Dia in Miles(max)',
    'Est Dia in Feet(min)', 'Est Dia in Feet(max)'
]

# Remove highly correlated orbital parameters
orbital_features_to_remove = [
    'Jupiter Tisserand Invariant',  # Correlated with Mean Motion & Orbital Period
    'Aphelion Dist',                # Correlated with Semi Major Axis
    'Mean Motion'                   # Correlated with Semi Major Axis & Orbital Period
]

# Combine all features to remove for Level 1
level1_features_to_remove = measurement_features_to_remove + orbital_features_to_remove

# Create Level 1 difficulty dataset
X_level1 = scaled_df.drop(columns=level1_features_to_remove, errors='ignore')

print("Level 1 Features Removed:")
for feature in level1_features_to_remove:
    print(f"- {feature}")
print(f"\nOriginal feature count: {scaled_df.shape[1]}")
print(f"Level 1 feature count: {X_level1.shape[1]}")
print(f"Features removed: {scaled_df.shape[1] - X_level1.shape[1]}")

# Add back any categorical columns you need
for col in categorical_cols:
    if col in df.columns:
        X_level1[col] = df[col]

# Display the first few rows of the resulting dataset
print("\nFirst 5 rows of Level 1 dataset:")
print(X_level1.head())

# Save this dataset for your models
X_level1.to_csv('nasa_asteroids_level1_difficulty.csv', index=False)
print("\nLevel 1 dataset saved to 'nasa_asteroids_level1_difficulty.csv'")

"""You've successfully implemented the first level of difficulty by removing 9 features:

Six redundant diameter measurements (in meters, miles, and feet)
Three highly correlated orbital parameters (Jupiter Tisserand Invariant, Aphelion Distance, and Mean Motion)

# - Identifying feature importance to create additional difficulty levels
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load the Level 1 dataset
df = pd.read_csv('nasa_asteroids_level1_difficulty.csv')

# Prepare data for feature importance analysis
X = df.copy()
y = df['OrbitalType']  # Using OrbitalType as target variable

# Remove target columns and non-numeric columns
target_columns = ['OrbitalType', 'RiskClass', 'ScientificPotential', 'Hazardous']
non_numeric = ['Close Approach Date', 'Orbiting Body', 'Orbit Determination Date', 'Equinox']
columns_to_remove = target_columns + non_numeric

for col in columns_to_remove:
    if col in X.columns:
        X = X.drop(col, axis=1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Train a Random Forest to get feature importance
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get feature importance
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
})
feature_importances = feature_importances.sort_values('Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importance for OrbitalType Classification', fontsize=16)
plt.tight_layout()
plt.savefig('feature_importance.png')  # Save the plot
plt.show()

print("Feature importance ranking:")
print(feature_importances)

# Create Level 2: Remove top 5 most important features
top_5_features = feature_importances.head(5)['Feature'].tolist()
X_level2 = X.drop(columns=top_5_features)

# Create Level 3: Remove top 10 most important features
top_10_features = feature_importances.head(10)['Feature'].tolist()
X_level3 = X.drop(columns=top_10_features)

# Create Level 4: Keep only 10 least important features
bottom_10_features = feature_importances.tail(10)['Feature'].tolist()
X_level4 = X[bottom_10_features]

# Print information about each level
print("\nDifficulty Levels Created:")
print(f"Level 1 (Current): {X.shape[1]} features")
print(f"Level 2 (- Top 5): {X_level2.shape[1]} features - Removed: {', '.join(top_5_features)}")
print(f"Level 3 (- Top 10): {X_level3.shape[1]} features - Removed: {', '.join(top_10_features)}")
print(f"Level 4 (Bottom 10 only): {X_level4.shape[1]} features - Kept only: {', '.join(bottom_10_features)}")

# Save datasets for each level
# Add back the target variables
for target in target_columns:
    if target in df.columns:
        X_level2[target] = df[target]
        X_level3[target] = df[target]
        X_level4[target] = df[target]

X_level2.to_csv('nasa_asteroids_level2_difficulty.csv', index=False)
X_level3.to_csv('nasa_asteroids_level3_difficulty.csv', index=False)
X_level4.to_csv('nasa_asteroids_level4_difficulty.csv', index=False)

print("\nAll difficulty level datasets saved to CSV files.")

"""Step 2 Results Analysis
Your feature importance analysis reveals some fascinating insights:

Top Most Important Features:

Semi Major Axis (0.254): This is the most important predictor, which makes perfect astronomical sense as it defines the size of an asteroid's orbit.
Eccentricity (0.245): The second most important feature measures how elliptical an orbit is.
Orbital Period (0.220): How long an asteroid takes to orbit the Sun is highly predictive.


Moderate Importance Features:

Perihelion Distance (0.104): Closest approach to the Sun
Inclination (0.034): Tilt of the orbit relative to Earth's orbital plane


Least Important Features:

ID fields (Neo Reference ID, Orbit ID)
Timing data (Epoch Date, Epoch Osculation)
Uncertainty measurements



This importance ranking aligns perfectly with astronomical theory - the fundamental orbital parameters (semi-major axis, eccentricity, and orbital period) are the most definitive features for classifying asteroid orbits.
Your Difficulty Levels Summary:

Level 1: 26 features (baseline with correlated features removed)
Level 2: 21 features (removed top 5 most important features)
Level 3: 16 features (removed top 10 most important features)
Level 4: 10 features (kept only the 10 least important features)

This creates an excellent progression of challenge levels. Level 4 will be particularly difficult as it uses only features with minimal predictive power.

# - Evaluate Model Performance
"""

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from imblearn.ensemble import BalancedRandomForestClassifier

# Function to evaluate a model on a specific difficulty level
def evaluate_model(model_name, model, X_train, X_test, y_train, y_test, difficulty_level):
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)

    # Return results
    return {
        'Model': model_name,
        'Difficulty': difficulty_level,
        'Accuracy': accuracy,
        'Feature Count': X_train.shape[1]
    }

# List of models to evaluate
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=100, random_state=42),
    'Balanced Random Forest': BalancedRandomForestClassifier(n_estimators=100, random_state=42)
}

# Load all datasets
levels = {
    'Level 1': pd.read_csv('nasa_asteroids_level1_difficulty.csv'),
    'Level 2': pd.read_csv('nasa_asteroids_level2_difficulty.csv'),
    'Level 3': pd.read_csv('nasa_asteroids_level3_difficulty.csv'),
    'Level 4': pd.read_csv('nasa_asteroids_level4_difficulty.csv')
}

# Target variable
target = 'OrbitalType'

# Store results
results = []

# Loop through each difficulty level
for level_name, df in levels.items():
    print(f"\nEvaluating models on {level_name}...")

    # Prepare data
    X = df.drop(columns=['OrbitalType', 'RiskClass', 'ScientificPotential', 'Hazardous',
                         'Close Approach Date', 'Orbiting Body', 'Orbit Determination Date',
                         'Equinox'], errors='ignore')
    y = df[target]

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Evaluate each model
    for model_name, model in models.items():
        print(f"  Training {model_name}...")
        result = evaluate_model(model_name, model, X_train, X_test, y_train, y_test, level_name)
        results.append(result)
        print(f"    Accuracy: {result['Accuracy']:.4f}")

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Visualize results
plt.figure(figsize=(14, 8))
sns.barplot(x='Difficulty', y='Accuracy', hue='Model', data=results_df)
plt.title('Model Performance Across Difficulty Levels', fontsize=16)
plt.xlabel('Difficulty Level', fontsize=14)
plt.ylabel('Accuracy', fontsize=14)
plt.ylim(0, 1)
plt.xticks(rotation=0)
plt.legend(title='Model', loc='lower left')
plt.tight_layout()
plt.savefig('model_performance_comparison.png')
plt.show()

# Print summary table
pivot_table = results_df.pivot_table(
    index='Model', columns='Difficulty', values='Accuracy', aggfunc='mean'
)
print("\nModel Performance Summary:")
print(pivot_table)

# Get feature counts
feature_counts = []
for level_name, df in levels.items():
    non_target_cols = ['OrbitalType', 'RiskClass', 'ScientificPotential', 'Hazardous',
                      'Close Approach Date', 'Orbiting Body', 'Orbit Determination Date', 'Equinox']
    feature_count = len([col for col in df.columns if col not in non_target_cols])
    feature_counts.append({'Level': level_name, 'Feature Count': feature_count})

feature_count_df = pd.DataFrame(feature_counts)
print("\nFeature Counts by Level:")
print(feature_count_df)

# Calculate performance drop
performance_drop = results_df.pivot_table(
    index='Model', columns='Difficulty', values='Accuracy', aggfunc='mean'
)
performance_drop['Level 1 to 4 Drop'] = performance_drop['Level 1'] - performance_drop['Level 4']
performance_drop['Drop Percentage'] = (performance_drop['Level 1 to 4 Drop'] / performance_drop['Level 1']) * 100

print("\nPerformance Drop Analysis:")
print(performance_drop[['Level 1', 'Level 4', 'Level 1 to 4 Drop', 'Drop Percentage']])

"""Performance Summary
Level 1 (26 features)

All models performed excellently with nearly perfect accuracy (≈99.8%)
Random Forest and Gradient Boosting tied for top performance
This confirms your baseline model is very strong when using all key features

Level 2 (21 features)

Significant performance drop after removing 5 most important features
Gradient Boosting maintained the best performance (70.0%)
Random Forest dropped to 66.8%
AdaBoost fell more dramatically to 58.5%

Level 3 (16 features)

Further performance degradation with only 16 features
Random Forest showed the most resilience (55.3%)
Gradient Boosting dropped to 54.2%
AdaBoost continued declining to 43.7%

Level 4 (10 features - most challenging)

Random Forest maintained the highest accuracy (52.3%)
Gradient Boosting fell to 49.1%
AdaBoost dropped to 40.2%
Balanced Random Forest collapsed to just 20.9%
Key Insights

Model Robustness to Feature Loss:

Random Forest proved most resilient with only a 47.5% performance drop from Level 1 to 4
Gradient Boosting showed similar robustness (50.7% drop)
AdaBoost struggled more (58.1% drop)
Balanced Random Forest performed worst (77.6% drop)


Feature Importance Validation:

The dramatic performance drop between Levels 1 and 2 (≈30%) confirms that the 5 features you removed (Semi Major Axis, Eccentricity, Orbital Period, Perihelion Distance, Inclination) were indeed critical
This validates your feature importance analysis


Ensemble Method Comparison:

Random Forest demonstrated the best ability to leverage weak predictors
Gradient Boosting excelled with moderate feature sets
Balanced Random Forest performed well only with complete feature sets

Practical Implications

For Real-World Applications:
When asteroid data might be incomplete or when some orbital parameters can't be measured precisely:

Random Forest would be the most reliable classifier
It maintains over 50% accuracy even with only the 10 weakest predictors


For Scientific Understanding:

Your results quantify exactly how important the primary orbital elements are
This confirms astronomical theory about which parameters most determine an asteroid's orbit type


For Future Work:

Consider testing hybrid models that might better handle missing top features
Explore feature engineering to create new composite features from the weaker predictors
Try more specialized ensemble methods that might perform better on Level 4 difficulty

What You've Completed

Data Processing:

- You've normalized and standardized your data using StandardScaler
- You've analyzed your data for missing values (finding none)
- You've implemented a comprehensive feature elimination strategy based on correlation analysis


Imbalance Analysis:

You've analyzed the class imbalance in your dataset
You've identified significant imbalance in the OrbitalType, RiskClass, and ScientificPotential classifications


Model Comparison:

You've compared four ensemble methods (Random Forest, Gradient Boosting, AdaBoost, Balanced Random Forest)
You've tested these models across four difficulty levels with progressively fewer features
You've analyzed how different models handle feature elimination



What You Still Need to Do

Apply Imbalance Mitigation Techniques:

Implement SMOTE, undersampling, or oversampling techniques
Compare model performance before and after applying these techniques
Analyze which imbalance mitigation approach works best with your dataset


Hyperparameter Tuning:

Use optuna for hyperparameter optimization (this is part of your extra work)
Implement a systematic search for optimal parameters for each ensemble method
Document the hyperparameter search process and results


Additional Metrics:

Expand your evaluation beyond accuracy to include recall, precision, F1 score
Implement Matthews Correlation Coefficient (MCC) as an additional metric (extra work)
Compare models using these comprehensive metrics


Complete Documentation:

Write up the final report with all required sections
Include visualizations and tables showing your results
Document the challenges faced and lessons learned

# - Implement SMOTE for imbalance mitigation:

This code:

Defines a comprehensive evaluation function that calculates multiple metrics (accuracy, precision, recall, F1, and MCC)
Loops through each difficulty level
For each level:

First evaluates all models without SMOTE (standard approach)
Then applies SMOTE to balance the classes
Evaluates all models again with the SMOTE-balanced training data


Creates visualizations comparing performance metrics between standard and SMOTE approaches
Generates summary tables showing how much SMOTE improved each model's performance

The output will help you understand:

How SMOTE affects model performance across different difficulty levels
Which models benefit most from class balancing
Whether SMOTE is more effective at higher difficulty levels (with fewer features)
"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Function to evaluate model with comprehensive metrics
def evaluate_model_comprehensive(model_name, model, X_train, X_test, y_train, y_test, difficulty_level, is_smote=False):
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    mcc = matthews_corrcoef(y_test, y_pred)

    method_type = "SMOTE" if is_smote else "Standard"

    # Return results
    return {
        'Model': model_name,
        'Difficulty': difficulty_level,
        'Method': method_type,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'MCC': mcc,
        'Feature Count': X_train.shape[1]
    }

# List of models to evaluate
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=100, random_state=42),
    'Balanced Random Forest': BalancedRandomForestClassifier(n_estimators=100, random_state=42)
}

# Load all datasets
levels = {
    'Level 1': pd.read_csv('nasa_asteroids_level1_difficulty.csv'),
    'Level 2': pd.read_csv('nasa_asteroids_level2_difficulty.csv'),
    'Level 3': pd.read_csv('nasa_asteroids_level3_difficulty.csv'),
    'Level 4': pd.read_csv('nasa_asteroids_level4_difficulty.csv')
}

# Target variable
target = 'OrbitalType'

# Store results
standard_results = []
smote_results = []

# Loop through each difficulty level
for level_name, df in levels.items():
    print(f"\nEvaluating models on {level_name}...")

    # Prepare data
    X = df.drop(columns=['OrbitalType', 'RiskClass', 'ScientificPotential', 'Hazardous',
                         'Close Approach Date', 'Orbiting Body', 'Orbit Determination Date',
                         'Equinox'], errors='ignore')
    y = df[target]

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 1. Standard evaluation without SMOTE
    print("  Standard evaluation (without SMOTE):")
    for model_name, model in models.items():
        print(f"    Training {model_name}...")
        result = evaluate_model_comprehensive(model_name, model, X_train, X_test, y_train, y_test, level_name, is_smote=False)
        standard_results.append(result)
        print(f"      Accuracy: {result['Accuracy']:.4f}, F1 Score: {result['F1 Score']:.4f}, MCC: {result['MCC']:.4f}")

    # 2. Apply SMOTE for imbalance mitigation
    print("  Applying SMOTE for class balance:")
    smote = SMOTE(random_state=42)
    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

    print(f"    Original class distribution: {pd.Series(y_train).value_counts().to_dict()}")
    print(f"    SMOTE class distribution: {pd.Series(y_train_smote).value_counts().to_dict()}")

    # Evaluate with SMOTE
    for model_name, model in models.items():
        print(f"    Training {model_name} with SMOTE...")
        result = evaluate_model_comprehensive(model_name, model, X_train_smote, X_test, y_train_smote, y_test, level_name, is_smote=True)
        smote_results.append(result)
        print(f"      Accuracy: {result['Accuracy']:.4f}, F1 Score: {result['F1 Score']:.4f}, MCC: {result['MCC']:.4f}")

# Combine results
all_results = standard_results + smote_results
results_df = pd.DataFrame(all_results)

# Visualize results comparing standard vs SMOTE
plt.figure(figsize=(20, 10))
for i, metric in enumerate(['Accuracy', 'F1 Score', 'MCC']):
    plt.subplot(1, 3, i+1)
    sns.barplot(x='Difficulty', y=metric, hue='Method', data=results_df)
    plt.title(f'{metric} by Difficulty Level and Method', fontsize=14)
    plt.xlabel('Difficulty Level', fontsize=12)
    plt.ylabel(metric, fontsize=12)
    plt.ylim(0, 1)
    plt.legend(title='Method')

plt.tight_layout()
plt.savefig('metrics_comparison_smote_vs_standard.png')
plt.show()

# Summary tables
for method in ['Standard', 'SMOTE']:
    subset = results_df[results_df['Method'] == method]
    pivot_acc = subset.pivot_table(index='Model', columns='Difficulty', values='Accuracy')
    pivot_f1 = subset.pivot_table(index='Model', columns='Difficulty', values='F1 Score')
    pivot_mcc = subset.pivot_table(index='Model', columns='Difficulty', values='MCC')

    print(f"\n{method} Method - Accuracy:")
    print(pivot_acc)

    print(f"\n{method} Method - F1 Score:")
    print(pivot_f1)

    print(f"\n{method} Method - MCC:")
    print(pivot_mcc)

# Calculate improvement from SMOTE
standard_df = results_df[results_df['Method'] == 'Standard']
smote_df = results_df[results_df['Method'] == 'SMOTE']

improvement = pd.merge(
    standard_df, smote_df,
    on=['Model', 'Difficulty'],
    suffixes=('_std', '_smote')
)

for metric in ['Accuracy', 'F1 Score', 'MCC']:
    improvement[f'{metric}_improvement'] = improvement[f'{metric}_smote'] - improvement[f'{metric}_std']
    improvement[f'{metric}_pct_improvement'] = (improvement[f'{metric}_improvement'] / improvement[f'{metric}_std']) * 100

improvement_summary = improvement.groupby(['Model', 'Difficulty'])[
    'Accuracy_improvement', 'F1 Score_improvement', 'MCC_improvement',
    'Accuracy_pct_improvement', 'F1 Score_pct_improvement', 'MCC_pct_improvement'
].mean().reset_index()

print("\nImprovement from SMOTE:")
print(improvement_summary)

"""# - Imbalance Analysis and Mitigation Techniques

Result:
"""

orbital_type_counts = df['OrbitalType'].value_counts(normalize=True) * 100
print("OrbitalType class distribution (%):")
print(orbital_type_counts)
risk_class_counts = df['RiskClass'].value_counts(normalize=True) * 100
print("RiskClass distribution (%):")
print(risk_class_counts)
scientific_potential_counts = df['ScientificPotential'].value_counts(normalize=True) * 100
print("ScientificPotential distribution (%):")
print(scientific_potential_counts)
hazardous_counts = df['Hazardous'].value_counts(normalize=True) * 100
print("Hazardous distribution (%):")
print(hazardous_counts)

"""Impact of Imbalance on Model Performance

- To quantify the effect of class imbalance on model performance, baseline models were evaluated with standard metrics and class-specific metrics:
"""

# Evaluate baseline model with imbalanced data
from sklearn.metrics import classification_report, confusion_matrix
# Get unique class labels
classes = np.unique(y_test)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Visualize confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.title('Confusion Matrix - Imbalanced Data')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

"""# - Estandarizar/normalizar características numéricas"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

""" Intenta convertir cada columna en flotante y quita las que fallan, lo que garantiza que solo queden columnas puramente numéricas para el escalado."""

# Alternative approach with more verification
numeric_features = df.copy()

# First, remove known target variables
for col in ['OrbitalType', 'RiskClass', 'ScientificPotential', 'Hazardous']:
    if col in numeric_features.columns:
        numeric_features = numeric_features.drop(col, axis=1)

# Then drop any column that has any non-numeric values
for col in numeric_features.columns.tolist():
    # Try to convert to float, drop column if it fails
    try:
        # Just testing conversion
        numeric_features[col].astype(float)
    except (ValueError, TypeError):
        print(f"Dropping non-numeric column: {col}")
        numeric_features = numeric_features.drop(col, axis=1)

# Verify remaining columns
print(f"Remaining columns for scaling: {numeric_features.columns.tolist()}")

# Now scale features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(numeric_features)
scaled_df = pd.DataFrame(scaled_features, columns=numeric_features.columns)

# Preview the scaled data
print(scaled_df.head())

"""Si necesita alguna de las columnas categóricas para su análisis después de escalar, puede volver a agregarlas:"""

# Add back categorical columns if needed for analysis
categorical_cols = ['OrbitalType', 'RiskClass', 'ScientificPotential', 'Hazardous']
for col in categorical_cols:
    if col in df.columns:
        scaled_df[col] = df[col]

"""1.-Shape verification:


*   Scaled shape: (4687, 35)
*   Original shape: (4687, 44)


This makes sense - you have the same number of rows (4687), but fewer columns in the scaled data because you removed the non-numeric columns.

2.-Distribution statistics:




*   The mean values for all scaled columns are very close to 0 (showing as values like 4.244761e-16, which is essentially 0)
*   The standard deviations are all very close to 1 (1.000107)

This is exactly what StandardScaler is supposed to do - transform each feature to have a mean of 0 and a standard deviation of 1. The tiny deviations from exactly 0 and 1 are just due to numerical precision.

"""

# Verify the shape
print(f"Original shape: {df.shape}")
print(f"Scaled shape: {scaled_df.shape}")

# Check distribution statistics
print("Distribution of scaled values:")
print(scaled_df.describe().loc[['mean', 'std']].T)

"""These scaled values make sense:

- Values near the mean become close to 0
- Values below the mean become negative
- Values above the mean become positive
- The magnitude tells you how many standard deviations from the mean


**EXAMPLE**


- the value 27.4 (index 3) became 1.775418 in the scaled data, indicating it's about 1.8 standard deviations above the mean.
- The value 20.3 (index 2) became -0.680766, indicating it's about 0.7 standard deviations below the mean


"""

# Compare a few values before and after scaling for one column
print("Original values for 'Absolute Magnitude':")
print(df['Absolute Magnitude'].head())

print("\nScaled values for 'Absolute Magnitude':")
print(scaled_df['Absolute Magnitude'].head())



"""# **3.- Class Imbalance Analysis and Mitigation**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from sklearn.model_selection import train_test_split

# Configurar estilo de visualización
plt.style.use('fivethirtyeight')
sns.set_palette('colorblind')

# 1. Analizar la distribución de clases para cada variable objetivo
target_variables = ['OrbitalType', 'RiskClass', 'ScientificPotential']

fig, axes = plt.subplots(len(target_variables), 1, figsize=(14, 5*len(target_variables)))

for i, target in enumerate(target_variables):
    if target in df.columns:
        # Contar la frecuencia de cada clase
        class_counts = df[target].value_counts()

        # Ordenar por frecuencia para mejor visualización
        class_counts = class_counts.sort_values(ascending=False)

        # Calcular porcentajes de clase
        class_percentages = 100 * class_counts / len(df)

        # Crear DataFrame para mejor presentación
        distribution_df = pd.DataFrame({
            'Count': class_counts,
            'Percentage': class_percentages
        })

        # Graficar
        ax = axes[i]
        bars = sns.barplot(x=class_counts.index, y=class_counts.values, ax=ax)
        ax.set_title(f'Distribución de Clases para {target}', fontsize=16)
        ax.set_ylabel('Número de muestras', fontsize=12)
        ax.set_xlabel('Clase', fontsize=12)
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')

        # Añadir etiquetas de porcentaje sobre cada barra
        for j, p in enumerate(bars.patches):
            height = p.get_height()
            ax.text(p.get_x() + p.get_width()/2., height + 50,
                   f'{class_percentages.iloc[j]:.1f}%',
                   ha="center", fontsize=10)

        # Imprimir la tabla de distribución
        print(f"\nDistribución de clases para {target}:")
        print(distribution_df)

        # Calcular ratio de desbalance (clase mayoritaria / clase minoritaria)
        imbalance_ratio = class_counts.iloc[0] / class_counts.iloc[-1]
        print(f"Ratio de desbalance (mayoritaria/minoritaria): {imbalance_ratio:.2f}\n")

plt.tight_layout()
plt.show()

# 2. Preparar los datos para aplicar técnicas de resampling

# Asegurarnos de que solo estamos usando características numéricas para SMOTE
X = scaled_df.copy()  # Tus características estandarizadas

# Verificar que no haya columnas no numéricas en X
for col in X.columns:
    try:
        X[col].astype(float)
    except (ValueError, TypeError):
        print(f"Eliminando columna no numérica de X: {col}")
        X = X.drop(col, axis=1)

print(f"Forma final de la matriz de características X: {X.shape}")

# 3. Aplicar diferentes técnicas de resampling para cada variable objetivo

# 3.1 Para OrbitalType
if 'OrbitalType' in df.columns:
    y = df['OrbitalType']

    # Dividir los datos para evitar data leakage
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print("\n==== Balanceo de clases para OrbitalType ====")
    print(f"Distribución original de clases: {sorted(Counter(y_train).items())}")

    # Visualizar distribución original
    plt.figure(figsize=(15, 10))
    plt.subplot(3, 1, 1)
    original_counts = pd.Series(Counter(y_train))
    sns.barplot(x=original_counts.index, y=original_counts.values)
    plt.title('Distribución Original de Clases')
    plt.ylabel('Recuento')
    plt.xticks(rotation=45, ha='right')

    # Técnica 1: SMOTE con estrategia personalizada (excluyendo la clase ultra-minoritaria)
    print("\n1. Aplicando SMOTE (excluyendo Outer-Solar):")
    sampling_strategy = {
        'Earth-Crosser': 1494,         # Mantener como está
        'Earth-Crosser-HighEcc': 1494, # Sobremuestrear para igualar clase mayor
        'MainBelt-High': 1494,         # Sobremuestrear para igualar clase mayor
        'MainBelt-Low': 1494,          # Sobremuestrear para igualar clase mayor
        'Mars-Crosser': 1494,          # Sobremuestrear para igualar clase mayor
        'Mars-Crosser-HighEcc': 1494   # Sobremuestrear para igualar clase mayor
        # Excluimos 'Outer-Solar' por tener muy pocas muestras
    }

    smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy)
    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

    print(f"Distribución después de SMOTE: {sorted(Counter(y_train_smote).items())}")
    print(f"Total de muestras: Original={len(y_train)}, Remuestreado={len(y_train_smote)}")

    # Visualizar después de SMOTE
    plt.subplot(3, 1, 2)
    smote_counts = pd.Series(Counter(y_train_smote))
    sns.barplot(x=smote_counts.index, y=smote_counts.values)
    plt.title('Distribución de Clases después de SMOTE')
    plt.ylabel('Recuento')
    plt.xticks(rotation=45, ha='right')

    # Técnica 2: SMOTETomek para un enfoque más balanceado
    print("\n2. Aplicando SMOTETomek con balance moderado:")

    # Obtenemos la distribución original
    original_distribution = Counter(y_train)

    # Definimos una estrategia que solo sobremuestrea las clases minoritarias
    target_counts = {}
    for class_name in original_distribution:
        if class_name == 'Earth-Crosser':
            # Mantener el número original para la clase mayoritaria
            target_counts[class_name] = original_distribution[class_name]
        elif class_name != 'Outer-Solar':  # Excluimos la clase ultra-minoritaria
            # Sobremuestrear clases minoritarias a un número razonable (por ejemplo, 800)
            target_counts[class_name] = 800

    print(f"Estrategia de muestreo para SMOTETomek: {target_counts}")

    smote_tomek = SMOTETomek(random_state=42, sampling_strategy=target_counts)
    X_train_tomek, y_train_tomek = smote_tomek.fit_resample(X_train, y_train)

    print(f"Distribución después de SMOTETomek: {sorted(Counter(y_train_tomek).items())}")
    print(f"Total de muestras: Original={len(y_train)}, Remuestreado={len(y_train_tomek)}")
    # Visualizar después de SMOTETomek
    plt.subplot(3, 1, 3)
    tomek_counts = pd.Series(Counter(y_train_tomek))
    sns.barplot(x=tomek_counts.index, y=tomek_counts.values)
    plt.title('Distribución de Clases después de SMOTETomek')
    plt.ylabel('Recuento')
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

    # Técnica 3: Enfoque para clase ultra-minoritaria (Outer-Solar)
    print("\n3. Enfoque especial para la clase Outer-Solar:")

    # Crear clasificador binario para Outer-Solar vs resto
    y_binary = y_train.apply(lambda x: 'Outer-Solar' if x == 'Outer-Solar' else 'Other')

    # Rebalancear este problema binario
    rus = RandomUnderSampler(sampling_strategy={
        'Outer-Solar': 5,   # Mantener todas las muestras
        'Other': 50         # Submuestrear mayoría a ratio razonable
    }, random_state=42)
    X_binary, y_binary = rus.fit_resample(X_train, y_binary)

    print(f"Distribución para clasificador binario: {sorted(Counter(y_binary).items())}")

    # Visualizar clasificador binario
    plt.figure(figsize=(10, 5))
    binary_counts = pd.Series(Counter(y_binary))
    sns.barplot(x=binary_counts.index, y=binary_counts.values)
    plt.title('Distribución para Clasificador Binario (Outer-Solar vs Others)')
    plt.ylabel('Recuento')
    plt.tight_layout()
    plt.show()

# 3.2 Para RiskClass (si está disponible en el dataset)
if 'RiskClass' in df.columns:
    y = df['RiskClass']

    # Dividir los datos para evitar data leakage
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print("\n==== Balanceo de clases para RiskClass ====")
    print(f"Distribución original de clases: {sorted(Counter(y_train).items())}")

    # Visualizar distribución original
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    original_counts = pd.Series(Counter(y_train))
    sns.barplot(x=original_counts.index, y=original_counts.values)
    plt.title('Distribución Original de Clases')
    plt.ylabel('Recuento')
    plt.xticks(rotation=45, ha='right')

    # Aplicar SMOTE para RiskClass
    smote = SMOTE(random_state=42)
    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

    print(f"Distribución después de SMOTE: {sorted(Counter(y_train_smote).items())}")

    # Visualizar después de SMOTE
    plt.subplot(1, 2, 2)
    smote_counts = pd.Series(Counter(y_train_smote))
    sns.barplot(x=smote_counts.index, y=smote_counts.values)
    plt.title('Distribución de Clases después de SMOTE')
    plt.ylabel('Recuento')
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

# 3.3 Para ScientificPotential (si está disponible en el dataset)
# Para ScientificPotential
if 'ScientificPotential' in df.columns:
    y = df['ScientificPotential']

    # Dividir los datos para evitar data leakage
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print("\n==== Balanceo de clases para ScientificPotential ====")
    print(f"Distribución original de clases: {sorted(Counter(y_train).items())}")

    # Visualizar distribución original
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    original_counts = pd.Series(Counter(y_train))
    sns.barplot(x=original_counts.index, y=original_counts.values)
    plt.title('Distribución Original de Clases')
    plt.ylabel('Recuento')
    plt.xticks(rotation=45, ha='right')

    # Identificar clases ultra-minoritarias (menos de 6 muestras)
    class_counts = Counter(y_train)
    ultra_minority_classes = [cls for cls, count in class_counts.items() if count < 6]

    if ultra_minority_classes:
        print(f"\nClases ultra-minoritarias detectadas: {ultra_minority_classes}")

        # Approach 1: Exclusión de clases ultra-minoritarias para SMOTE
        print("\n1. Aplicando SMOTE excluyendo clases ultra-minoritarias:")

        # Crear una estrategia de muestreo personalizada
        sampling_strategy = {}
        for cls, count in class_counts.items():
            if cls not in ultra_minority_classes:  # Excluir clases ultra-minoritarias
                sampling_strategy[cls] = max(class_counts.values())  # Igualar a la mayor clase

        # Aplicar SMOTE
        smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy)
        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

        print(f"Distribución después de SMOTE: {sorted(Counter(y_train_smote).items())}")

        # Visualizar después de SMOTE
        plt.subplot(1, 2, 2)
        smote_counts = pd.Series(Counter(y_train_smote))
        sns.barplot(x=smote_counts.index, y=smote_counts.values)
        plt.title('Distribución después de SMOTE (excluyendo ultra-minoritarias)')
        plt.ylabel('Recuento')
        plt.xticks(rotation=45, ha='right')

        # Approach 2: Enfoque de clasificación jerárquica para clases ultra-minoritarias
        print("\n2. Enfoque de clasificación jerárquica para clases ultra-minoritarias:")

        # Crear conjunto de datos para clasificación binaria
        y_binary = y_train.apply(lambda x: 'Peculiar' if x == 'Peculiar' else 'No-Peculiar')

        # Balancear este problema binario con submuestreo
        from imblearn.under_sampling import RandomUnderSampler

        rus = RandomUnderSampler(sampling_strategy={
            'Peculiar': class_counts['Peculiar'],  # Mantener todas las muestras
            'No-Peculiar': 40                      # Submuestrear a una cantidad razonable
        }, random_state=42)
        X_binary, y_binary = rus.fit_resample(X_train, y_binary)

        print(f"Distribución para clasificador binario: {sorted(Counter(y_binary).items())}")

        # Visualizar clasificador binario
        plt.figure(figsize=(8, 4))
        binary_counts = pd.Series(Counter(y_binary))
        sns.barplot(x=binary_counts.index, y=binary_counts.values)
        plt.title('Distribución para Clasificador Binario (Peculiar vs No-Peculiar)')
        plt.ylabel('Recuento')
        plt.tight_layout()

    else:
        # Si no hay clases ultra-minoritarias, aplicar SMOTE normal
        print("\nAplicando SMOTE estándar:")
        smote = SMOTE(random_state=42, k_neighbors=3)  # Reducir k_neighbors si es necesario
        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

        print(f"Distribución después de SMOTE: {sorted(Counter(y_train_smote).items())}")

        # Visualizar después de SMOTE
        plt.subplot(1, 2, 2)
        smote_counts = pd.Series(Counter(y_train_smote))
        sns.barplot(x=smote_counts.index, y=smote_counts.values)
        plt.title('Distribución después de SMOTE')
        plt.ylabel('Recuento')
        plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

"""Alternativamente, puedes intentar un enfoque más simple reduciendo el parámetro k_neighbors en SMOTE:"""

# Solución rápida: Reducir k_neighbors
print("\nAplicando SMOTE con k_neighbors reducido:")
smote = SMOTE(random_state=42, k_neighbors=min(min(Counter(y_train).values())-1, 3))
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(f"Distribución después de SMOTE: {sorted(Counter(y_train_smote).items())}")

